# Installation

first, get python, you can use conda or venv as well.

run the quick installation code

```
python quick_installation.py
```

the UI notebook has a min example.

# ChatMyArchive


# Motivation: something that works, can be verified, on private sources,

 - search for law files, lots of pdf, special structure, wording/phrasing, Claude refusing, GPT doesn't answer
 - 1. want to be **correct, referenced for very sure (otherwise get sued for publishing fake law/ news or plagarism, I mean, the AI does not get prisoned, users, could)** Good guesses are good, but this is law journalism, serious content, answers **can be validified by people** 2. but people can only read finite content **when we validate it, we don't spend even more time**
 - run locally, accessibility, privacy, customizable


# Lessons& perspective

 - this looks obsolete and artificial? although real application
 - relevance in topic vs relevance in content
 - MoEs can be fast! We can(and must) waste power for speed. Can we free ride in the future?
 - pdf mining (ToC), KeyBert, summarizatin, Hierachical RAG, CoT, ToT (too much to run)

# Demo


# Reference

some important part this uses:

 - langchain
 - faiss
 - react agent
 - sentence transformers All mini LM v6
 - Mixtral 8x7b
